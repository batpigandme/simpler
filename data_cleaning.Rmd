# Cleaning and manipulating data

So we've imported our data. From a database, from an Excel file, from a CSV - wherever it came from, we have it. We've even looked through it to see if there's any interesting nastiness. Then what?

The answer is usually data cleaning. Exploratory analysis is great for getting the low-hanging fruit in imperfect data, and for understanding where the thornier problems lie, but we then have to clean *up* those thorny problems. That's where manipulating and cleaning data comes in.

## Reshaping data

One essential part of cleaning is *reshaping* your data's structure. Sometimes as researchers we are lucky enough to have data that looks exactly like what we want. Every row is a single observation. Every column contains one, and just one, thing we care about. This is referred to as *tidy data* (a concept you can read more about [here](http://vita.had.co.nz/papers/tidy-data.html)).

We'll use the original patient dataset as an example. Let's take a look at it:

```{r, eval=FALSE}
patient_data <- read_csv(file = "https://www.ironholds.org/resources/misc/patient_data.csv")
head(patient_data)

  year facility_number                  facility type_of_control age_group admitted released
1 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     10-19       62        2
2 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     20-29      124       63
3 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     30-39      140       67
4 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     40-49      243      122
5 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     50-59      411      172
6 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     60-69      415      343
```

There are a few ways in which this data is 'untidy'. First-off, the facility and facility *location* are in the same column - we might want to have them as different fields. Second, we've got multiple types of data (patient admissions, and patient releases) in different columns. This is good for some operations, like calculating percentages, but can be a tremendous pain with others. We may need to tidy the data, perform operations on it, untidy it for others, and then back again.

So we want to be able to split a column, merge patient admissions and patient releases into one key-value structure, and learn how to un-merge it just in case we have to. We can do all of these with the `tidyr` package.

First: column-splitting. We've got that `facility` field, containing both the facility name and location, and we want two columns (`facility_name` and `facility_location`). For that we can use the `separate` function from `tidyr`, which does just that: separates the contents of a specified column, based on a user-defined separator, into multiple new columns (with user-specified names). Let's use it now:

```{r, eval=FALSE}
library(tidyr)

split_data <- separate(data = patient_data, col = facility, into = c("facility_name", "facility_location"),
                       sep = ", ")

head(split_data)

   year facility_number    facility_name facility_location type_of_control age_group admitted released
  <int>           <chr>            <chr>             <chr>           <chr>     <chr>    <int>    <int>
1  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     10-19       62        2
2  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     20-29      124       63
3  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     30-39      140       67
4  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     40-49      243      122
5  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     50-59      411      172
6  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     60-69      415      343
```

We've split `facility` into `facility_name` and `facility_location`. Pretty neat! Still leaves us with what to do about the multiple value columns, though. The answer comes in the form of `gather`, another tidyr function, which takes a data frame, the names to give the key and value columns, and columns to treat as `id` columns that aren't touched. It then splits the rest:

```{r, eval=FALSE}

gathered_data <- gather(data = split_data, key = "type", value = "count", admitted, released)

head(gathered_data)
   year facility_number    facility_name facility_location type_of_control age_group     type count
  <int>           <chr>            <chr>             <chr>           <chr>     <chr>    <chr> <int>
1  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     10-19 admitted    62
2  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     20-29 admitted   124
3  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     30-39 admitted   140
4  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     40-49 admitted   243
5  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     50-59 admitted   411
6  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     60-69 admitted   415
```

If we want to reverse that operation, we can use `spread`:

```{r, eval=FALSE}

spread_data <- spread(data = gathered_data, key = type, value = count)

head(spread_data)

   year facility_number    facility_name facility_location type_of_control age_group admitted released
  <int>           <chr>            <chr>             <chr>           <chr>     <chr>    <int>    <int>
1  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     10-19       62        2
2  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     20-29      124       63
3  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     30-39      140       67
4  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     40-49      243      122
5  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     50-59      411      172
6  2009          010735 ALAMEDA HOSPITAL           ALAMEDA        District     60-69      415      343
```

...and at that point, we're right back where we started.

## Data manipulation

Beyond tidying data, another common need is to aggregate it. Raw data, split by as many variables as possible, gives us the widest possible amount of information, but that doesn't mean it's easy to visualise or understand (`patient_data` has 23,578 observations, for example. Nobody wants a 23,000 point graphic).

Data manipulation is the practice of reshaping, aggregating or splitting data to make it easier to use for a particular task. If you're exploring data, that might look like aggregating it at a very high level to get a general feel for what things look like. If you're visualising it, that might look like calculating particular metrics for entire groups of observations, and getting a data frame out at the end you can turn into a graph. In these cases, and a wide range of other cases, the answer is another R package: `dplyr`.

`dplyr` provides a "grammar" of data manipulation: a set of functions that can be strung together into almost a sentence that describes what's being done to the data. For example, if we wanted to summarise, for each facility, the number of admitted patients in the 10-19 age group, we would `filter` the dataset to 10-19, `group` the entries by facility name, and `summarize` the dataset to get the mean.

This language-like construct is aided by a new operator that we'll introduce: `%>%`, or the "pipe" operator. Normally, R consists of a series of statements, assigning the results to objects, and passing those objects to *new* statements, and so on:

```{r, eval=FALSE}
start_data <- "foo"
result_of_first_step <- do_thing(start_data)
result_of_second_step <- do_second_thing(result_of_first_step)
final_result <- do_third_thing(result_of_second_step)
```

The pipe operator lets us "chain" operations together; instead of assigning to an object between function calls, you can forward the results of each function call to the first argument in the next function:

```{r, eval=FALSE}
start_data <- "foo"
final_result <- do_thing(start_data) %>%
                  do_second_thing %>%
                  do_third_thing
```

The code is much cleaner (and somewhat faster to run!), and when you add the "language"-like aspect of dplyr and tidyr in, it becomes almost a sentence. It's also much easier to modify; if you want to add a new thing between the second and third things, you can just add it, and don't have to worry about changing names and assignments and all the rest.

Let's demonstrate by taking the patient data dataset, and then working out the average number of 10-19 year olds admitted to facilities a year:

```{r, eval=FALSE}
facility_admission_average <- read_csv(file = "https://www.ironholds.org/resources/misc/patient_data.csv") %>%
  filter(age_group == "10-19") %>%
  group_by(year) %>%
  summarise(admission_average = mean(admitted))

head(facility_admission_average)

   year admission_average
  <int>             <dbl>
1  2009          418.5859
2  2010          407.1007
3  2011          390.5659
4  2012          378.8952
5  2013          356.8860
6  2014          350.0652
```

And just like that we've gone from raw data to averaged hospital admissions, by year, for a particular age group - although we probably should have rounded the results. I'm not sure if you can admit half a patient; certainly not without a rather grim prognosis and, in the event that they survive, medical bills that cost an arm and a leg.

Dad jokes aside, when you combine `tidyr`, `dplyr` and the pipe operator, you can write long pieces of data cleaning and manipulation code that just flow naturally: they're practically (gasp) human-readable! Both of those packages can do a lot more, and if you're interested in a more dedicated exploration of their features, I thoroughly recommend Bradley Boehmke's [data wrangling guide](https://rpubs.com/bradleyboehmke/data_wrangling).

## Text-cleaning

Numbers are easy, because numbers are, well, just numbers. Text is a lot harder, because text tends to be the result of human input, and humans introduce typos. The result can throw off your data analysis - you might be dealing with information about two groups of patients, "admitted" and "released", but the data could very well also include "ADMITTED", and "relesad" (which is, to be fair, really sad).

Summarising the data won't work, since it'll be grouping by values that are actually totally meaningless, graphing will be a tremendous pain for similar reasons, and eventually you'll be sat there manually going through and clearing up the information, which will take slightly longer than the amount of time you have before your manager sends you the next dataset.

So instead, let's learn how to automatically clean up text in R. For this we'll need the `stringr` package, which is a package that lets you (as the name suggests) manipulate strings.

If we look at the `patient_data` dataset, there are a few things that make the text awkward. For one thing, all the strings are IN ALL CAPS. Really we want them to be formatted like proper nouns (since that's what they are). We can use `stri_to_title` to do that:

```{r, eval=FALSE}
library(stringr)
patient_data <- read_csv(file = "https://www.ironholds.org/resources/misc/patient_data.csv")
patient_data$facility <- str_to_title(patient_data$facility)

   year facility_number                  facility type_of_control age_group admitted released
  <int>           <chr>                     <chr>           <chr>     <chr>    <int>    <int>
1  2009          010735 Alameda Hospital, Alameda        District     10-19       62        2
2  2009          010735 Alameda Hospital, Alameda        District     20-29      124       63
3  2009          010735 Alameda Hospital, Alameda        District     30-39      140       67
4  2009          010735 Alameda Hospital, Alameda        District     40-49      243      122
5  2009          010735 Alameda Hospital, Alameda        District     50-59      411      172
6  2009          010735 Alameda Hospital, Alameda        District     60-69      415      343
```

Much cleaner! And `stringr` has a lot of other functions we can apply, too, including:

1. `str_trim` for getting rid of extraneous whitespace (`"foo"` and `"foo "` are the same thing, but R doesn't know that);
2. `str_order`, for sorting a vector of strings;
3. Applying regular expressions to identify matches (`str_match`), replace matches (`str_replace`) or extract them (`str_extract`). Regular expressions are discussed [here](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) and pretty complex, so may require further reading to dig into and understand.

## Dates and times

Although it's not relevant for *this* dataset, dates and times commonly crop up as vector types in R. They're important - if you have longitudinal data, you rely on dates or date-times to segment your data. Time-series data, even moreso!

While base R has some functions for handling dates and times, they're not particularly user-friendly and are mostly useful in emergencies. Still, sometimes you have an emergency, so let's go through them.

The big one is `strptime`, which lets you turn arbitrary(ish) dates and times, formatted as a string, into a proper date or datetime object. All it needs is a string, and a format for that string, and it goes off and does its thing. That format can feature full or abbreviated month names, or month numbers, or 12-hour time, or 24-hour time - anything. Check out the documentation at `?strptime` to see the full list.

Let's demonstrate with the most awkward possible date-time I can imagine, and take it from a string to a date-time to just a date.

```{r, eval=FALSE}
# 7 March 2016, at 9pm, 32 minutes and 51 seconds
awkward_datetime <- "16-Mar/07:09-32-51 pm"
datetime <- strptime(x = awkward_datetime, format = "%y-%b/%d:%I-%M-%S %p")
str(datetime)

POSIXlt[1:1], format: "2016-03-07 21:32:51"
  
# That's a date/time (POSIXlt and/or POSIXct). To turn it into a date...
date <- as.Date(datetime)
str(date)

Date[1:1], format: "2016-03-07"
```

`strptime` is very useful because it can handle pretty much anything you throw at it, but most of the time dates tend to be formatted in a fairly similar way: YYYY-MM-DD, or YYYY-MM-DD HH:MM:SS, say. That's where `lubridate` comes in: it contains functions which act as convenient shorthands, so you don't have to remember precisely what all the little keywords are for formatting each time:

```{r, eval=FALSE}
library(lubridate)

date <- "01-12-2015"
date_time <- "01-12-2015 16:43:12"

dmy(date)
[1] "2015-12-01"

dmy_hms(date_time)
[1] "2015-12-01 16:43:12 UTC"
```

It also has `mdy`, `ydm` and `ymd`-related sets of functions for dates or times that follow one of those formats. But wait, there's more!

Sometimes you want dates, or date-times. Sometimes, though, you *don't* - you just want information included in them. For example, getting just the year, or second, or quarter.

This is another area where `lubridate` can help us out: once you've got something formatted and converted into a date or date-time, you can grab chunks of it using functions named after those chunks. With the examples above, for example:

```{r, eval=FALSE}

date_time <- dmy_hms("01-12-2015 16:43:12")

year(date_time)
[1] 2015

quarter(date_time)
[1] 4

second(date_time)
[1] 12
```

And there are functions for every other time element you could want too; month, day, hour, minute, even week or weekday (`wday`). So lubridate can be tremendously useful when it comes to reformatting date-times, which is going to be vital when we're trying to visualise data as time series'.

