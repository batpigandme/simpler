[
["index.html", "Starting Out with R and Shiny Preamble", " Starting Out with R and Shiny Oliver Keyes Preamble Welcome to Starting Out with R and Shiny, a guide to learning basic R and ending up with the ability to put together Shiny dashboards - online, R-based dashboards and visualisations. This set of documentation is being written for a set of lectures in Springfield, IL, and is primarily aimed at attendees, but it will hopefully be useful to others too. It contains (or will contain): An introduction to R as a language, its history, and its structure; A basic guide to RStudio and online help resources for questions, bugs or commentary; Importing data into R from databases or flat files; Cleaning and reshaping that data with tidyr, dplyr, stringi, lubridate and related tools; Shiny as a data visualisation platform; Miscellaneous more-advanced tips and tricks on writing and running R and Shiny in a reliable manner. "],
["introduction-to-r.html", "1 Introduction to R 1.1 A brief history of splines 1.2 Why R succeeds 1.3 The R community 1.4 R installation 1.5 Using RStudio 1.6 Tutorial format", " 1 Introduction to R 1.1 A brief history of splines The R programming language started off at the University of Auckland, where two statistics professors - Ross Ihaka and Robert Gentleman - found themselves somewhat perturbed by the state of statistical programming. There were a lot of statistical languages and environments out there, but most of the ones they had access to were commercial, requiring you to pay to use them. In the end they decided to write something themselves, and they called it R (first because it’s the first letter in both of their names and second, I hypothesise, because after you’ve written an entire programming language with one other person you probably don’t have the energy left to come up with a fancy name). It was initially used just by the two of them in teaching courses at Auckland, and student complaints were the main source of improvements. In June 1995, though, everything changed when they released the source code of the language online - and did so under a free license, which let other people reuse the code and crucially modify the code. People began submitting their own improvements, new features, and bug fixes, and the language grew and grew. By 2012 it was the most commonly used data science language - a status it continues to maintain to this day.ref 1.2 Why R succeeds R didn’t become popular in a vacuum; there are a lot of other solutions for data analysis out there, some commercial (SAS, SPSS) and some open source (MATLAB, Python, Julia). Despite that, R consistently outstrips them in popularity, and there are a few reasons for that. R was written by statisticians. A lot of programming languages (including statistical ones) are written, unsurprisingly, by programmers - and this is great from a software engineering standpoint. R, however, was written by statisticians, and while this means the language looks slightly strange compared to more “proper” programming languages, it means that it’s designed from the ground up to handle the things that statisticians and scientists need. The primary data formats are tabular, just like the results from scientific experiments tend to be; the language has not only arithmetic support, but also significance tests, general modelling functions and matrix mathematics built in. R was written by a community. As well as the base language, which is the work of many hands, there’s also an entire ecosystem of add-ons (which we’ll cover later). Put simply: if there’s something statistical you need to do, and it isn’t in R itself, it’s almost certainly in an add-on - one that you can obtain, use and include in your work completely free of charge. R can do pretty much everything. It’s primarily a statistical language, but it’s not just a statistical language. You can do data analysis in it, sure. You can also visualise the results of that analysis in a thousand different ways. Or write reports based on it that turn out as LaTeX, PDFs, HTML, Markdown. or build dynamic, web-hosted visualisations and dashboards and interactive platforms to display your work. Heck, you can even build websites in it; I did just to see if I could, and it took 40 lines of code. Although in that specific case, while you could use R, I would strongly recommend you don’t. The point, though, is that while R is primarily about statistics, it’s a language that covers pretty much every step of handling data, from reading it in and cleaning it up to showing your results to the world. R can interact with pretty much everything. That’s a slight exaggeration, but only slight. Suppose someone gives you code written in C? R can interact with that. FORTRAN? Yep. C++? Absolutely! And while we’re at it, you can link R into Python, JavaScript, Java, Scala, and a whole host of other languages. This means that on the off-chance you encounter something you really need to do that isn’t in R, you can integrate it in pretty easily. There are a lot of other fascinating and wonderful features (some of which we’ll cover later on!) but those are the main ones, and why after writing thousands upon thousands of lines of R in the last few years, I’m still so in love with it. 1.3 The R community One of the things I touched on above - as a reason R is such a powerful and popular environment - is the community. A big benefit is the expertise that community has: for example, the R add-on for neural networks was written by Brian Ripley, a British statistician who quite literally wrote the book on neural networks, and having the person who invented the methodology implement it gives you a pretty strong claim to the implementation being reliable. But another benefit is simply that, while there are dragons in some corners, the community generally consists of people who love expanding what the language can do and how friendly to new users it is - and they practise what they preach! And there are a lot of R programmers, and a lot of places to get help, support, suggestions and pointers. Some venues worth keeping in mind, in case you run into a problem and need some assistance, are: R on StackOverflow; R’s official mailing lists, and; the R IRC channel. 1.4 R installation To get started with R, you’re going to need…well, R! If you’re on Windows you can get R here; if you’re on a mac, you can download it here. For Linux users (specifically Ubuntu users, or people on other, Debian-based distributions) open the terminal and type: sudo apt-get -y install r-base This requires the password to your computer’s administrator account; if you don’t have it, you’ll need to get the system administrator to install R. Once you’ve got R, you’ll also need RStudio - an Integrated Development Environment (IDE) for R that we’ll be covering shortly. RStudio can be obtained here. After that, you’ll need a few R packages (more on those later) that are used in this tutorial. Open RStudio, now that you’ve installed it, and go to the window in the bottom left marked “console”. Type: install.packages(&quot;readr&quot;, &quot;dplyr&quot;, &quot;stringr&quot;, &quot;tidyr&quot;, &quot;readxl&quot;, &quot;shinydashboard&quot;) (Don’t worry about what this means right now; we’ll get to it later) 1.5 Using RStudio You should now have RStudio; that’s going to be your primary interface for writing R in. Let’s briefly step through the important bits of the program that you should be paying attention to. When you open it up there should be three different panes, nestled inside the program, looking something like this: Let’s step through each of them in turn. Going clockwise, we first have the console: this is a window into which you can type R commands, and see the output - we’ll be using that a lot. Next over, we’ve got a pane with two tabs; “Environment” (the default) and “History”. “Environment” contains a listing of everything you create during your R tinkering, while “History” provides, well, a history, of the commands you’ve typed into the console: that way if you need to re-run something, you can find it easily. And, last but not least down in the bottom right, we have “Files” (which shows your filesystem, so you can find R scripts you’ve already saved or datasets you want to read in) , “Plots”, which will contain any visualisations you generate so that you can prototype them, “Packages”, which lists all the R add-ons on your system (and which are loaded), and “Help”, which shows any documentation you request. In practice, another tab will open up when you have an R script open, where the top of “console” is now, and this will contain whatever file(s) you’re editing. For now, though, let’s focus on the console. 1.6 Tutorial format From hereonin we’re going to be focusing on using R, rather than the history, community and everything else. One approach to teaching a programming language is to step through the different rules in how the language is constructed, what the different keywords are, any strange quirks it has, and all the rest - and this can be really helpful for people who are already programmers. That isn’t guaranteed, though, and so the rest of this book is divided up into individual, practical applications of the language, combined with explanations of what’s going on in each piece so that you can learn the structure of the language through doing. So, going forward, you’ll see something that looks like the following; I’ll introduce a concept, or a step in analysing data, and talk about why it’s important. And then, after a colon, because I flagrantly abuse both the colon and semicolon: There&#39;ll be a chunk of code, in a shaded box just like this one, which will achieve the step I was talking about After that will be an explanation of that chunk of code, and what each bit is doing and how everything in it works. There are also likely to be lines starting with #: these are comments, which aren’t treated as code and run, but instead let you document what the code is doing within your R program. If you’re the sort of person who instead wants to jump into the nitty-gritty of the language, I recommend reading Hadley Wickham’s Advanced R Programming (specifically the data structures section) to get a handle for the general workings of R, and then coming back here to learn the applications. "],
["exploratory-data-analysis.html", "2 Exploratory Data Analysis 2.1 Dataset metadata 2.2 Missing and duplicate values 2.3 Examining values", " 2 Exploratory Data Analysis So you’ve got your dataset and read it in. Great! But is it what you expect it to be? A key stage in data processing and analysis is Exploratory Data Analysis, or EDA. This is where you’re tinkering with the data not with a specific goal in mind, but simply to ensure that it’s what you’re looking for, that it’s formatted nicely enough for further analysis, and to understand what kinds of analysis you can actually do with it. This part of the guide will cover EDA, with the goal of teaching you some convenient shorthands to identify when data is corrupt or badly formatted. What precisely to do about that is covered in the next part, which is about data cleaning and manipulation. 2.1 Dataset metadata How many observations and variables are there? Which variables are they? How is the dataset structured? Being able to answer these questions is essential for identifying if you’re missing information, if the information is invalid, or how to write R analysing the data you’ve got, and all of them can be answered with metadata. We’ve already seen the str function, which presents a summary of the structure of an object; individual elements of that summary can also be grabbed on a one-off basis, which is useful when you just need a specific thing and not the visual overload of “everything about this dataset ever”. ncol and nrow get the number of columns and rows from a data frame (and length gets the size of a vector, or list), while class gets the object type: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: chr &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... nrow(patient_data) [1] 23578 ncol(patient_data) [1] 7 length(patient_data$year) [1] 23578 class(patient_data) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(patient_data$year) [1] &quot;integer&quot; This is also useful, of course, for things like tests of statistical significance or power; you can easily grab the number of observations you’re running your tests over. We can use names to get the column names of a data frame: we can also use it to change those names, which is tremendously useful for cleanup when you have an input dataset with inconsistent or incoherent labels: names(patient_data) [1] &quot;year&quot; &quot;facility_number&quot; &quot;facility&quot; &quot;type_of_control&quot; &quot;age_group&quot; &quot;admitted&quot; &quot;released&quot; names(patient_data) &lt;- c(&quot;year&quot;, &quot;facility_number&quot;, &quot;facility&quot;, &quot;control&quot;, &quot;age_group&quot;, &quot;admitted&quot;, &quot;released&quot;) str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: chr &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ control : chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... Knowing all of these functions, we can get a pretty good sense of how a dataset is structured (making analysis much easier) and if it’s missing anything (making passing it back to the person who gave it to you, with a refined list of requirements, before you waste time analysing it, easier!) 2.2 Missing and duplicate values Beyond the dataset as a whole, it’s often valuable to look at individual values: specifically, to look at whether any are missing or duplicative. Just because a dataset has as many observations and variables as you expect doesn’t mean it’s perfect, just that it’s not obviously flawed. In R, missing values are represented by NA (Not Applicable), NaN (Not a Number) or NULL (…null). NaN only appears in numeric and integer types, since it’s really not news that strings aren’t numbers. Let’s take an example vector, and look at various ways of handling NA values: example_vector &lt;- c(1, 2, 3, NA, 4, NA, 5) # is.na() produces a vector of TRUE or FALSE values, where TRUE # indicates that the equivalent entry in example_vector is an NA is.na(example_vector) [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE # We can then subset the example_vector to include these entries, # or exclude these entries: nas_only &lt;- example_vector[is.na(example_vector)] nas_only [1] NA NA non_nas &lt;- example_vector[!is.na(example_vector)] non_nas [1] 1 2 3 4 5 # We can also overwrite NAs with &quot;empty&quot; values so they don&#39;t get in the way of data processing example_vector[is.na(example_vector)] &lt;- 0 example_vector [1] 1 2 3 0 4 0 5 This works for NANs (is.nan) and NULLs (is.null) too; we can identify them, exclude them (or everything else), and overwrite them with a value of our choice. So we can handle missing values. What about duplicates? For that we turn to duplicated: example_vector &lt;- c(1, 2, 3, 4, 1) duplicated(example_vector) [1] FALSE FALSE FALSE FALSE TRUE # We can perform the same operations (subsetting, overwriting) as with is.na and its sister functions. example_vector &lt;- example_vector[!duplicated(example_vector)] example_vector [1] 1 2 3 4 # Another thing we can do is both cases is pull out the row or element numbers for duplicated elements, # so we can inspect them visually. which(duplicated(example_vector)) [1] 5 example_vector[5] [1] 1 All of these functions work on data frames as well as vectors, so you can look for (for example) duplicate rows rather than having to check for duplicates against every single vector inside a dataset, and then subset to just that row using df[row_number,] instead of vector[entry]. 2.3 Examining values So we’ve got around the right number of observations, and we’ve handled the missing values. Let’s dig into the dataset itself! When you’ve got a categorical variable, it’s valuable to be able to see the unique categories within it (and how the data is distributed between those categories). We can do this with the unique and table functions, respectively: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) unique(patient_data$year) [1] 2009 2010 2011 2012 2013 2014 table(patient_data$year) 2009 2010 2011 2012 2013 2014 4004 3962 3851 3934 3926 3901 So we can see there are 4004 observations where year is 2009, 3962 where year is 2010, and so on. We can also use table to look at this information for permutations of multiple variables: table(patient_data$year, patient_data$age_group) 10-19 1-09 20-29 30-39 40-49 50-59 60-69 70-79 80+ Under 1 Unknown 2009 425 265 436 432 434 434 434 425 415 280 24 2010 417 260 433 427 430 430 429 418 407 281 30 2011 410 256 421 417 418 414 416 403 390 272 34 2012 420 243 435 430 430 427 428 417 399 275 30 2013 421 235 431 430 429 429 427 420 404 276 24 2014 414 233 430 427 425 424 423 419 400 276 30 With all of this put together, we can take a dataset that’s been provided and dig around in it a bit to find (and handle) missing information, examine whether the dataset meets our expectations, and quickly identify if there are deficits before we spend a load of energy analysing it. "],
["data-import-and-export.html", "3 Data Import and Export 3.1 Importing from CSVs 3.2 Importing Excel spreadsheets 3.3 Other files 3.4 Database import 3.5 Exporting data", " 3 Data Import and Export Obviously the first step in data analysis is getting the data, so let’s start there! Datasets come in lots of different formats, so we’ll cover the ones you most commonly see in a business context, namely: Comma-separated and tab-separated values files (CSV/TSV); Microsoft Excel spreadsheets; MySQL and similar databases. 3.1 Importing from CSVs CSVs and TSVs are the most common file format for data, and the good news there is that as a consequence, R has a whole host of tools for reading them in. Let’s open by loading one of them: the readr add-on I had you install earlier: library(readr) So, what’s this code doing? First, library; that’s a function, which is a self-contained chunk of code that’s packaged and made available so you can use it as many times as you want without having to write the code out entirely each time. readr is the input provided to library. So it works out as do_this_thing(to_this_thing). The library function loads packages - the add-ons I was talking about earlier - so you can refer to the code they contain when you’re working. In this case it’s loading readr, a package specifically designed for reading in and writing out a wide range of file formats, including CSVs and TSVs. When you see a function name, you can usually access documentation about it; remember the help pane I mentioned earlier? If you type the function name into the console, preceded by ?, the help pane will pop up with documentation for that function - what it’s used for, what values it accepts, and generally how it works. So we have our file-reading code. Now let’s read in the file, with: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) Let’s break down what’s going on in that code, going from right to left. First we have a piece of text - a URL, in fact - in quote marks. The quote marks identify the text as a string; something that R should treat just as text, not as code to be executed. Then we have that URL being associated with “file”, the entire thing wrapped in parentheses, preceded by read_csv which, as we’ve seen from loading the readr package, is a function call. So what we have is a call to run read_csv, with the “file” argument being that URL - so it’s reading a CSV from that URL! The argument, as the name file suggests, doesn’t have to be a URL; it can also be a file on your local computer. Then we shift left and run into &lt;-. That’s R’s assignment operator; it assigns the results of whatever code is on the right of the operator, to a name on the left of it (in this case, patient_data). So the code, in plain English, is “read in the CSV at this URL and assign the contents of it to the name patient_data”. If the file was a TSV, the process would look exactly the same; you’d just use read_tsv instead of read_csv. We call the result (the output of code, tied to a name) an object. You can see the contents of an object just by typing its name into the console, but that shows the entire thing, and this dataset is rather large; instead, let’s use the head function, which just shows the top few rows (the head of the data): head(patient_data) year facility_number facility type_of_control age_group admitted released 1 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 60-69 415 343 As you can see, it’s tabular, just like the CSV - columns of values, rows of observations, and row numbers (from 1 to 6). What you’re looking at is a “data frame”, the most common type of object in R: it’s designed to hold tabular data, since tabular data is what most statisticians and data scientists rely on. We can get a more detailed look at it with the str function (which means “structure” and does exactly what it says on the tin): str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: chr &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... There’s a lot going on here, so let’s break it down: First, “classes”; this is analogous to the object type. We don’t have to dig into exactly how the system works (although feel free to ask me about it via email or do further research); the important thing to note is that one of the classes is data.frame, identifying this as, well, a data frame! We then have “23578 obs. of 7 variables”; there are 23,578 rows, and 7 columns (which, in R, are known as vectors - more on those later). You can get these bits of information distinctly, through the nrow and ncol functions. We then have a sort of flipped version of the head output - showing each vector, from the first to the last, down. There are the vector names (year, facility_number, facility), examples of the entries in those columns, and then these little strings “int” and “chr”. These represents the “type” of the column - the kind of data it can contain. int vectors contain whole numbers; chr (or “character”) vectors contain strings. Other common types to see are logical (or logi), which contains true and false values, and numeric/num columns, which contain non-whole numbers. So now we have our data read into R. It’s not quite perfect, though; the facility_number vector is meant to be, well, a column of numbers, but according to str it’s actually a character vector. This offers us an opportunity to explore modifying data frames, and switching between types. One of the reasons data frames are powerful is that you can access, change and use individual vectors, as well as the data frame as a whole; this is done by calling, instead of data_frame_name, data_frame_name$column_name. Distinctly from that, it is possible to change the type of an object in R (the process is known as coercing), which can be very useful; depending on what you’re doing, you might want to store a value as a different type. Coersion is done with the as functions; as.numeric to turn something into a numeric value, as.integer for an integer, and then as.logical and as.character for those respective types. Let’s use both techniques now, to clear up that facility_number vector: patient_data$facility_number &lt;- as.integer(patient_data$facility_number) str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: int 10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... As you can see, facility_number is now an integer vector, which lets us (amongst other things) do maths with it if we so choose - tremendously useful. Vectors are very powerful in their own right, and a useful object type for when you don’t want to be dealing with an entire data frame. They can be created in a couple of ways: # This creates a vector from scratch, using data in the code: a_vector &lt;- c(1,2,3,4,5) # This creates a vector from a data frame column patient_data_counts &lt;- patient_data$count Individual elements of a vector can be accessed using vectorname[n], where n is the Nth element. So if you want the fifth element from some_vector: some_vector[5]. We can add new vectors to data frames using a technique that is similar to the one we used to modify them. We can also take advantage of a feature of R called “vectorisation”, which is most-easily thought about with an Excel analogy. You have your Excel spreadsheet, and you create a formula in one cell to add up all the values in that row. You want to do that for every row, so you drag the formula down the column and it copies the formula out, ending up with something like: A1 + A2 B1 + B2 C1 + C2 Vectorisation is just that - except it’s automated. No dragging and dropping required! Most R functions automatically work on entire vectors of data, and work when you have a vectors on one side but only a single value on the other. Let’s combine that with data frame modification to work out the percentage of the total patients admission number in each row: patient_data$percentage &lt;- (patient_data$admitted / sum(patient_data$admitted)) * 100 str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 8 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: int 10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... $ percentage : num 0.000266 0.000532 0.000601 0.001043 0.001764 ... No drag and drop necessary; the patient_data data frame has a new column, percentage, which contains (for each row) the admitted count divided by the total count of admitted patients for the entire dataset, which we retrieved with sum. So we now know: How to read data into R from CSV files (and TSV files!); How to examine the structure of an R object; How do access individual columns of a data frame; How to modify those columns, and create new ones; How vectors work; How vectorisation works. Let’s move on to everyone’s favourite business data format: Microsoft Excel! 3.2 Importing Excel spreadsheets Microsoft Excel is widely used in a business context, and correspondingly the data it outputs (.xls and .xlsx files) are widely distributed. We should learn how to read them into R, which is a big step in shifting away from Excel-based workflows and ensures that you can consume data from more traditional sources without running into format incompatibilities. For this we’ll need another R package, readxl, which does just that - reads in Excel files. Let’s load it and read in a file that the package comes with (which we can find with the system.file function): library(readxl) excel_data &lt;- read_excel(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) head(excel_data) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa It’s another data frame - as expected, since Excel spreadsheets are tabular just like CSV files are. One important note - Excel spreadsheets can contain multiple sheets, each of them a table. How do we handle that? If you look at the documentation for read_excel, you’ll see a sheet argument, which you can use to specify which sheet to read in. The excel_sheets function, meanwhile, gets you how many sheets an Excel file has. So we can combine them to work out how many sheets we’re looking at, and then read in that many. But what happens when you have to write code that reads in a variety of different files, some with different numbers of sheets? How do we write code that doesn’t mind how many sheets there are? How do we store the results of that code? A few new concepts and pieces of R, for these questions. First is how we’re going to store the results. We’re looking at an unknown number of sheets, which means an unknown number of data frames. Luckily, data frames can be stored as components of a single object, just like vectors are components of data frames. That means instead of having an unknown number of objects, we just have one. For that kind of storage, we’re going to introduce a new object type in R: lists. Lists can have any length you want, and contain pretty much any object you want, too, which makes them extremely powerful. You can construct and use them with, or without names. # A list without names nameless_list &lt;- list(2, 3) # A list with names named_list &lt;- list(first_object = 2, second_object = 3) The difference is in how you get access, or alter, objects inside the list. If the list has names, you can use the $ character we use with data frame contents. If it doesn’t, you have to use indices - getting the Nth element of the object by pointing to [[N]]. Taking the list examples we created above: nameless_list[[1]] [1] 2 named_list$first_object [1] 2 (you can actually use indices in named lists, too, but the inverse isn’t true. Plus, why use opaque numbers when you’ve taken the time to give all the elements nice human-friendly labels?) So now we know how to store the results. But how do we get them? First, we need to have a way of automatically knowing how many sheets there are. We can do that with another readxl function, excel_sheets, which lists all the sheets in an Excel file: sheet_names &lt;- excel_sheets(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) sheet_names [1] &quot;iris&quot; &quot;mtcars&quot; &quot;chickwts&quot; &quot;quakes&quot; Then we need a way of taking that vector of sheet names, and, for each one, reading in that sheet, and storing all the results in a list. The answer is the lapply function. lapply takes a list (or vector), X and, for each element of X, performs a specified operation using it. The results of each operation on each element is then returned in a list. Perfect! So what we want is: sheet_names &lt;- excel_sheets(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) all_sheets &lt;- lapply(X = sheet_names, FUN = read_excel, path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) If you run str on the results of that, what you should see is that all_sheets is a list, containing 4 data frames, all of which contain a pile of columns and rows just like our example at the beginning. These data frames can be extracted using the indices operation; all_sheets[[1]] gets you the first sheet, and so on, and so forth. So now we’ve learned how to read in Excel files. We’ve also learned: What lists are (and how they can be used to store a variety of objects); How to apply a function to each element of a list (or vector); How to get the results of that application. 3.3 Other files There are a few other file types you might encounter. One of the most common is tab-separated value files (TSVs); another, less common, is fixed-width files, where there’s an arbitrary amount of space between elements on each row to make sure every entry is the same width. 3.4 Database import To connect to a MySQL database, we’ll use the RMySQL package (as you may be noticing, R package developers love putting the letter R in their package names). As the name suggests, it integrates R with MySQL, providing code that lets you connect to a MySQL database and read information from it, turning said information into a data frame along the way. This process actually has quite a few steps: you need to: # Connect to a database database_connection &lt;- dbConnect(RMySQL::MySQL(), host = &quot;ensembldb.ensembl.org&quot;, user = &quot;anonymous&quot;, table = NULL) # Send a query response &lt;- dbSendQuery(database_connection, &quot;SHOW DATABASES&quot;) # Fetch the results of that query results &lt;- dbFetch(response) # Clear the results on the server end dbClearResult(response) # Close the connection dbDisconnect(database_connection) head(results) Database 1 information_schema 2 aedes_aegypti_core_48_1b 3 aedes_aegypti_core_49_1b 4 aedes_aegypti_core_50_1c 5 aedes_aegypti_core_51_1c 6 aedes_aegypti_core_52_1d That’s complicated, and also a total pain if you have to do it more than once - that’s a lot of code to write out every time, when all you’re really likely to be modifying is the query, or maybe table, for that particular host. The good news is that programmers are fundamentally lazy creatures, and so have worked out a way around this: “a pile of code that I keep having to repeat with only minor changes” is exactly what a function is. So now we’re going to learn how to create a function - one with two parameters, database and query, which will handle all of the commands we used above, just modifying database and query to whatever the user calling the function specifies. Defining a function, in some ways, looks a lot like defining any other object: you assign the output of a command to a name, it’s just that in this case the “output” is a chunk of code: times_two &lt;- function(x){ output &lt;- x * 2 return(output) } times_two(x = 10) [1] 20 There are only two new things here: the bit in curly braces is the body of the function - the code that the function contains, that is executed when you call the function. The return statement at the end, which is also new, simply tells the function “and here is the object you give back to the user when you are done”. So for database connecting, we need to wrap all the code we used in the database example into a function that accepts db and query as arguments, and returns the results of the database query. Simple enough: query_database &lt;- function(db, query){ # Make a connection database_connection &lt;- dbConnect(RMySQL::MySQL(), host = &quot;ensembldb.ensembl.org&quot;, user = &quot;anonymous&quot;, db = db) # Send a query response &lt;- dbSendQuery(database_connection, query) # Fetch the results of that query results &lt;- dbFetch(response) # Clear the results on the server end dbClearResult(response) # Close the connection dbDisconnect(database_connection) # Return the results! return(results) } tables &lt;- query_database(db = &quot;aedes_aegypti_core_51_1c&quot;, query = &quot;SHOW TABLES&quot;) head(tables) Tables_in_aedes_aegypti_core_51_1c 1 alt_allele 2 analysis 3 analysis_description 4 assembly 5 assembly_exception 6 attrib_type And we can keep reusing query_database with whatever database (or query!) we want. 3.5 Exporting data A related topic to data import - albeit, one you won’t have to use quite yet - is data export. It’s pretty common to need to save the results of whatever work you’re doing, either so you can pick it up later, hand it off to other people, or use it in other processes. There are a couple of ways to do that. If you’re saving simple, tabular data - data frames, say - we can use the readr package again. As well as read_csv and read_tsv, there’s also write_csv and write_tsv. : patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) write_csv(x = patient_data, file = &quot;patient_data.csv&quot;) You should now have a file called “patient_data.csv” in whatever directory you’re working from (which you can find out with the wd function). It’s properly formatted and can be read back into R, or into Excel, or into whatever tool you choose. Obviously, non-tabular objects (such as lists) can’t be saved into a CSV: how would R, or any other program, know what to do with them? This can present a problem when trying to share work with others, or save it for your own reuse. Luckily, R has its own data format, .RData files, which can store representations of any R object you want and then be loaded seamlessly back into R (the limitation is it probably won’t work with other languages or software). These files can also contain however many objects you want, of many different types. They’re created with the save function: a_list &lt;- list(a = 12, b = 6) a_vector &lt;- c(1, 2, 3, 4) save(a_list, a_vector, file = &quot;an_rdata_file.RData&quot;) You should now have an_rdata_file.Rdata saved to disk. It can be read in with the load function, which will restore the objects with whatever names they had when they were saved. "],
["cleaning-and-manipulating-data.html", "4 Cleaning and manipulating data 4.1 Reshaping data 4.2 Data manipulation 4.3 Text-cleaning 4.4 Dates and times", " 4 Cleaning and manipulating data So we’ve got our data read in. From a database, from an Excel file, from a CSV - wherever it came from, we have it. Then what? The answer is usually data cleaning. People tend to act as if analysis is 50% studious statistical testing and 50% visualisation and reporting, but this is a damned dirty lie. In practice, it’s 80% cleaning the data so it will work, 20% statistics, and a further 20% visualisation - summing to over 100 because you didn’t realise quite how much of a pain the cleaning would be. 4.1 Reshaping data One essential part of cleaning is reshaping your data’s structure. Sometimes as researchers we are lucky enough to have data that looks exactly like what we want. Every row is a single observation. Every column contains one, and just one, thing we care about. This is referred to as tidy data (a concept you can read more about here). We’ll use the original patient dataset as an example. Let’s take a look at it: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) head(patient_data) year facility_number facility type_of_control age_group admitted released 1 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 60-69 415 343 There are a few ways in which this data is ‘untidy’. First-off, the facility and facility location are in the same column - we might want to have them as different fields. Second, we’ve got multiple types of data (patient admissions, and patient releases) in different columns. This is good for some operations, like calculating percentages, but can be a tremendous pain with others. We may need to tidy the data, perform operations on it, untidy it for others, and then back again. So we want to be able to split a column, merge patient admissions and patient releases into one key-value structure, and learn how to un-merge it just in case we have to. We can do all of these with the tidyr package. First: column-splitting. We’ve got that facility field, containing both the facility name and location, and we want two columns (facility_name and facility_location). For that we can use the separate function from tidyr, which does just that: separates the contents of a specified column, based on a user-defined separator, into multiple new columns (with user-specified names). Let’s use it now: library(tidyr) split_data &lt;- separate(data = patient_data, col = facility, into = c(&quot;facility_name&quot;, &quot;facility_location&quot;), sep = &quot;, &quot;) head(split_data) year facility_number facility_name facility_location type_of_control age_group admitted released &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 415 343 We’ve split facility into facility_name and facility_location. Pretty neat! Still leaves us with what to do about the multiple value columns, though. The answer comes in the form of gather, another tidyr function, which takes a data frame, the names to give the key and value columns, and columns to treat as id columns that aren’t touched. It then splits the rest: gathered_data &lt;- gather(data = split_data, key = &quot;type&quot;, value = &quot;count&quot;, admitted, released) head(gathered_data) year facility_number facility_name facility_location type_of_control age_group type count &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 admitted 62 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 admitted 124 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 admitted 140 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 admitted 243 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 admitted 411 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 admitted 415 If we want to reverse that operation, we can use spread: spread_data &lt;- spread(data = gathered_data, key = type, value = count) head(spread_data) year facility_number facility_name facility_location type_of_control age_group admitted released &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 415 343 …and at that point, we’re right back where we started. 4.2 Data manipulation Beyond tidying data, another common need is to aggregate it. Raw data, split by as many variables as possible, gives us the widest possible amount of information, but that doesn’t mean it’s easy to visualise (patient_data has 23,578 observations, for example. Nobody wants a 23,000 point graphic) or understand. Data manipulation is the practice of reshaping, aggregating or splitting data to make it easier to use for a particular task. If you’re exploring data, that might look like aggregating it at a very high level to get a general feel for what things look like. If you’re visualising it, that might look like calculating particular metrics for entire groups of observations, and getting a data frame out at the end you can turn into a graph. In these cases, and a wide range of other cases, the answer is another R package: dplyr. dplyr provides a “grammar” of data manipulation: a set of functions that can be strung together into almost a sentence that describes what’s being done to the data. For example, if we wanted to summarise, for each facility, the number of admitted patients in the 10-19 age group, we would filter the dataset to 10-19, group the entries by facility name, and summarize the dataset to get the mean. This language-like construct is aided by a new operator that we’ll introduce: %&gt;%, or the “pipe” operator. Normally, R consists of a series of statements, assigning the results to objects, and passing those objects to new statements, and so on: start_data &lt;- &quot;foo&quot; result_of_first_step &lt;- do_thing(start_data) result_of_second_step &lt;- do_second_thing(result_of_first_step) final_result &lt;- do_third_thing(result_of_second_step) The pipe operator lets us “chain” operations together; instead of assigning to an object between function calls, you can forward the results of each function call to the first argument in the next function: start_data &lt;- &quot;foo&quot; final_result &lt;- do_thing(start_data) %&gt;% do_second_thing %&gt;% do_third_thing The code is much cleaner (and somewhat faster to run!), and when you add the “language”-like aspect of dplyr and tidyr in, it becomes almost a sentence. It’s also much easier to modify; if you want to add a new thing between the second and third things, you can just add it, and don’t have to worry about changing names and assignments and all the rest. Let’s demonstrate by taking the patient data dataset, and then working out the average number of 10-19 year olds admitted to facilities a year: facility_admission_average &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) %&gt;% filter(age_group == &quot;10-19&quot;) %&gt;% group_by(year) %&gt;% summarise(admission_average = mean(admitted)) head(facility_admission_average) year admission_average &lt;int&gt; &lt;dbl&gt; 1 2009 418.5859 2 2010 407.1007 3 2011 390.5659 4 2012 378.8952 5 2013 356.8860 6 2014 350.0652 And just like that we’ve gone from raw data to averaged hospital admissions, by year, for a particular age group - although we probably should have rounded the results. I’m not sure if you can admit half a patient; certainly not without a rather grim prognosis and, in the event that they survive, medical bills that cost an arm and a leg. Dad jokes aside, when you combine tidyr, dplyr and the pipe operator, you can write long pieces of data cleaning and manipulation code that just flow naturally: they’re practically (gasp) human-readable! Both of those packages can do a lot more, and if you’re interested in a more dedicated exploration of their features, I thoroughly recommend Bradley Boehmke’s data wrangling guide. 4.3 Text-cleaning Numbers are easy, because numbers are, well, just numbers. Text is a lot harder, because text tends to be the result of human input, and humans introduce typos. The result can throw off your data analysis - you might be dealing with information about two groups of patients, “admitted” and “released”, but the data could very well also include “ADMITTED”, and “relesad” (which is, to be fair, really sad). Summarising the data won’t work, since it’ll be grouping by values that are actually totally meaningless, graphing will be a tremendous pain for similar reasons, and eventually you’ll be sat there manually going through and clearing up the information, which will take slightly longer than the amount of time you have before your manager sends you the next dataset. So instead, let’s learn how to automatically clean up text in R. For this we’ll need the stringr package, which is a package that lets you (as the name suggests) manipulate strings. If we look at the patient_data dataset, there are a few things that make the text awkward. For one thing, all the strings are IN ALL CAPS. Really we want them to be formatted like proper nouns (since that’s what they are). We can use stri_to_title to do that: library(stringr) patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) patient_data$facility &lt;- str_to_title(patient_data$facility) year facility_number facility type_of_control age_group admitted released &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 2009 010735 Alameda Hospital, Alameda District 10-19 62 2 2 2009 010735 Alameda Hospital, Alameda District 20-29 124 63 3 2009 010735 Alameda Hospital, Alameda District 30-39 140 67 4 2009 010735 Alameda Hospital, Alameda District 40-49 243 122 5 2009 010735 Alameda Hospital, Alameda District 50-59 411 172 6 2009 010735 Alameda Hospital, Alameda District 60-69 415 343 Much cleaner! We can also use stringr to clean up some of the strangeness we saw in the exploratory data analysis - the extraneous spaces that appear in some (otherwise identical) strings, with str_trim: 4.4 Dates and times "],
["building-your-first-app.html", "5 Building your first app", " 5 Building your first app "],
["user-interface-design.html", "6 User Interface design", " 6 User Interface design "],
["custom-inputs-and-outputs.html", "7 Custom inputs and outputs", " 7 Custom inputs and outputs "],
["reactive-expressions.html", "8 Reactive expressions", " 8 Reactive expressions "],
["shiny-dashboarding.html", "9 Shiny dashboarding", " 9 Shiny dashboarding "],
["code-style.html", "10 Code style", " 10 Code style "],
["version-control.html", "11 Version control", " 11 Version control "],
["testing-and-staging.html", "12 Testing and staging", " 12 Testing and staging "]
]
