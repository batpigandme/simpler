[
["index.html", "Starting Out with R and Shiny Preamble", " Starting Out with R and Shiny Oliver Keyes Preamble Welcome to Starting Out with R and Shiny, a guide to learning basic R and ending up with the ability to put together Shiny dashboards - online, R-based dashboards and visualisations. This set of documentation is being written for a set of lectures in Springfield, IL, and is primarily aimed at attendees, but it will hopefully be useful to others too. It contains (or will contain): An introduction to R as a language, its history, and its structure; A basic guide to RStudio and online help resources for questions, bugs or commentary; Importing data into R from databases or flat files; Cleaning and reshaping that data with tidyr, dplyr, stringi, lubridate and related tools; Shiny as a data visualisation platform; Miscellaneous more-advanced tips and tricks on writing and running R and Shiny in a reliable manner. "],
["background.html", "1 Background 1.1 A brief history of splines 1.2 Why R succeeds 1.3 The R community", " 1 Background 1.1 A brief history of splines The R programming language started off at the University of Auckland, where two statistics professors - Ross Ihaka and Robert Gentleman - found themselves somewhat perturbed by the state of statistical programming. There were a lot of statistical languages and environments out there, but most of the ones they had access to were commercial, requiring you to pay to use them. In the end they decided to write something themselves, and they called it R (first because it’s the first letter in both of their names and second, I hypothesise, because after you’ve written an entire programming language with one other person you probably don’t have the energy left to come up with a fancy name). It was initially used just by the two of them in teaching courses at Auckland, and student complaints were the main source of improvements. In June 1995, though, everything changed when they released the source code of the language online - and did so under a free license, which let other people reuse the code and crucially modify the code. People began submitting their own improvements, new features, and bug fixes, and the language grew and grew. By 2012 it was the most commonly used data science language - a status it continues to maintain to this day.ref 1.2 Why R succeeds R didn’t become popular in a vacuum; there are a lot of other solutions for data analysis out there, some commercial (SAS, SPSS) and some open source (MATLAB, Python, Julia). Despite that, R consistently outstrips them in popularity, and there are a few reasons for that. R was written by statisticians. A lot of programming languages (including statistical ones) are written, unsurprisingly, by programmers - and this is great from a software engineering standpoint. R, however, was written by statisticians, and while this means the language looks slightly strange compared to more “proper” programming languages, it means that it’s designed from the ground up to handle the things that statisticians and scientists need. The primary data formats are tabular, just like the results from scientific experiments tend to be; the language has not only arithmetic support, but also significance tests, general modelling functions and matrix mathematics built in. R was written by a community. As well as the base language, which is the work of many hands, there’s also an entire ecosystem of add-ons (which we’ll cover later). Put simply: if there’s something statistical you need to do, and it isn’t in R itself, it’s almost certainly in an add-on - one that you can obtain, use and include in your work completely free of charge. R can do pretty much everything. It’s primarily a statistical language, but it’s not just a statistical language. You can do data analysis in it, sure. You can also visualise the results of that analysis in a thousand different ways. Or write reports based on it that turn out as LaTeX, PDFs, HTML, Markdown. or build dynamic, web-hosted visualisations and dashboards and interactive platforms to display your work. Heck, you can even build websites in it; I did just to see if I could, and it took 40 lines of code. Although in that specific case, while you could use R, I would strongly recommend you don’t. The point, though, is that while R is primarily about statistics, it’s a language that covers pretty much every step of handling data, from reading it in and cleaning it up to showing your results to the world. R can interact with pretty much everything. That’s a slight exaggeration, but only slight. Suppose someone gives you code written in C? R can interact with that. FORTRAN? Yep. C++? Absolutely! And while we’re at it, you can link R into Python, JavaScript, Java, Scala, and a whole host of other languages. This means that on the off-chance you encounter something you really need to do that isn’t in R, you can integrate it in pretty easily. There are a lot of other fascinating and wonderful features (some of which we’ll cover later on!) but those are the main ones, and why after writing thousands upon thousands of lines of R in the last few years, I’m still so in love with it. 1.3 The R community One of the things I touched on above - as a reason R is such a powerful and popular environment - is the community. A big benefit is the expertise that community has: for example, the R add-on for neural networks was written by Brian Ripley, a British statistician who quite literally wrote the book on neural networks, and having the person who invented the methodology implement it gives you a pretty strong claim to the implementation being reliable. But another benefit is simply that, while there are dragons in some corners, the community generally consists of people who love expanding what the language can do and how friendly to new users it is - and they practise what they preach! And there are a lot of R programmers, and a lot of places to get help, support, suggestions and pointers. Some venues worth keeping in mind, in case you run into a problem and need some assistance, are: R on StackOverflow; R’s official mailing lists, and; the R IRC channel. "],
["getting-set-up.html", "2 Getting set up 2.1 Installation 2.2 Using RStudio 2.3 Tutorial format", " 2 Getting set up 2.1 Installation To get started with R, you’re going to need…well, R! If you’re on Windows you can get R here; if you’re on a mac, you can download it here. For Linux users (specifically Ubuntu users, or people on other, Debian-based distributions) open the terminal and type: sudo apt-get -y install r-base This requires the password to your computer’s administrator account; if you don’t have it, you’ll need to get the system administrator to install R. Once you’ve got R, you’ll also need RStudio - an Integrated Development Environment (IDE) for R that we’ll be covering shortly. RStudio can be obtained here. After that, you’ll need a few R packages (more on those later) that are used in this tutorial. Open RStudio, now that you’ve installed it, and go to the window in the bottom left marked “console”. Type: install.packages(&quot;readr&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readxl&quot;, &quot;shinydashboard&quot;, &quot;magrittr&quot;) (Don’t worry about what this means right now; we’ll get to it later) 2.2 Using RStudio You should now have RStudio; that’s going to be your primary interface for writing R in. Let’s briefly step through the important bits of the program that you should be paying attention to. When you open it up there should be three different panes, nestled inside the program, looking something like this: Let’s step through each of them in turn. Going clockwise, we first have the console: this is a window into which you can type R commands, and see the output - we’ll be using that a lot. Next over, we’ve got a pane with two tabs; “Environment” (the default) and “History”. “Environment” contains a listing of everything you create during your R tinkering, while “History” provides, well, a history, of the commands you’ve typed into the console: that way if you need to re-run something, you can find it easily. And, last but not least down in the bottom left, we have “Files” (which shows your filesystem, so you can find R scripts you’ve already saved or datasets you want to read in) , “Plots”, which will contain any visualisations you generate so that you can prototype them, “Packages”, which lists all the R add-ons on your system (and which are loaded), and “Help”, which shows any documentation you request. In practice, another tab will open up when you have an R script open, where the top of “console” is now, and this will contain whatever file(s) you’re editing. For now, though, let’s focus on the console. 2.3 Tutorial format From hereonin we’re going to be focusing on using R, rather than the history, community and everything else. One approach to teaching a programming language is to step through the different rules in how the language is constructed, what the different keywords are, any strange quirks it has, and all the rest - and this can be really helpful for people who are already programmers. That isn’t guaranteed, though, and so the rest of this book is divided up into individual, practical applications of the language, combined with explanations of what’s going on in each piece so that you can learn the structure of the language through doing. So, going forward, you’ll see something that looks like the following; I’ll introduce a concept, or a step in analysing data, and talk about why it’s important. And then, after a colon, because I flagrantly abuse both the colon and semicolon: There&#39;ll be a chunk of code, in a shaded box just like this one, which will achieve the step I was talking about After that will be an explanation of that chunk of code, and what each bit is doing and how everything in it works. If you’re the sort of person who instead wants to jump into the nitty-gritty of the language, I recommend Hadley Wickham’s Advanced R Programming (specifically the data structures section) to get a handle for the general workings of R, and then coming back here to learn the applications. "],
["data-import-and-export.html", "3 Data Import and Export 3.1 Importing from CSVs 3.2 Importing Excel spreadsheets 3.3 Other files 3.4 Database import 3.5 Exporting data", " 3 Data Import and Export Obviously the first step in data analysis is getting the data, so let’s start there! Datasets come in lots of different formats, so we’ll cover the ones you most commonly see in a business context, namely: Comma-separated and tab-separated values files (CSV/TSV); Microsoft Excel spreadsheets; MySQL and similar databases. 3.1 Importing from CSVs CSVs and TSVs are the most common file format for data, and the good news there is that as a consequence, R has a whole host of tools for reading them in. Let’s open by loading one of them: the readr add-on I had you install earlier: library(readr) So, what’s this code doing? First, library; that’s a function, which is a self-contained chunk of code that’s packaged and made available so you can use it as many times as you want without having to write the code out entirely each time. readr is the input provided to library. So it works out as do_this_thing(to_this_thing). The library function loads packages - the add-ons I was talking about earlier - so you can refer to the code they contain when you’re working. In this case it’s loading readr, a package specifically designed for reading in and writing out a wide range of file formats, including CSVs and TSVs. When you see a function name, you can usually access documentation about it; remember the help pane I mentioned earlier? If you type the function name into the console, preceded by ?, the help pane will pop up with documentation for that function - what it’s used for, what values it accepts, and generally how it works. So we have our file-reading code. Now let’s read in the file, with: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) Let’s break down what’s going on in that code, going from right to left. First we have a piece of text - a URL, in fact - in quote marks. The quote marks identify the text as a string; something that R should treat just as text, not as code to be executed. Then we have that URL being associated with “file”, the entire thing wrapped in parentheses, preceded by read_csv which, as we’ve seen from loading the readr package, is a function call. So what we have is a call to run read_csv, with the “file” argument being that URL - so it’s reading a CSV from that URL! The argument, as the name file suggests, doesn’t have to be a URL; it can also be a file on your local computer. Then we shift left and run into &lt;-. That’s R’s assignment operator; it assigns the results of whatever code is on the right of the operator, to a name on the left of it (in this case, patient_data). So the code, in plain English, is “read in the CSV at this URL and assign the contents of it to the name patient_data”. If the file was a TSV, the process would look exactly the same; you’d just use read_tsv instead of read_csv. We call the result (the output of code, tied to a name) an object. You can see the contents of an object just by typing its name into the console, but that shows the entire thing, and this dataset is rather large; instead, let’s use the head function, which just shows the top few rows (the head of the data): head(patient_data) year facility_number facility type_of_control age_group admitted released 1 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 60-69 415 343 As you can see, it’s tabular, just like the CSV - columns of values, rows of observations, and row numbers (from 1 to 6). What you’re looking at is a “data frame”, the most common type of object in R: it’s designed to hold tabular data, since tabular data is what most statisticians and data scientists rely on. We can get a more detailed look at it with the str function (which means “structure” and does exactly what it says on the tin): str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: chr &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; &quot;010735&quot; ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... There’s a lot going on here, so let’s break it down: First, “classes”; this is analogous to the object type. We don’t have to dig into exactly how the system works (although feel free to ask me about it via email or do further research); the important thing to note is that one of the classes is data.frame, identifying this as, well, a data frame! We then have “23578 obs. of 7 variables”; there are 23,578 rows, and 7 columns (which, in R, are known as vectors - more on those later). You can get these bits of information distinctly, through the nrow and ncol functions. We then have a sort of flipped version of the head output - showing each vector, from the first to the last, down. There are the vector names (year, facility_number, facility), examples of the entries in those columns, and then these little strings “int” and “chr”. These represents the “type” of the column - the kind of data it can contain. int vectors contain whole numbers; chr (or “character”) vectors contain strings. Other common types to see are logical (or logi), which contains true and false values, and numeric/num columns, which contain non-whole numbers. So now we have our data read into R. It’s not quite perfect, though; the facility_number vector is meant to be, well, a column of numbers, but according to str it’s actually a character vector. This offers us an opportunity to explore modifying data frames, and switching between types. One of the reasons data frames are powerful is that you can access, change and use individual vectors, as well as the data frame as a whole; this is done by calling, instead of data_frame_name, data_frame_name$column_name. Distinctly from that, it is possible to change the type of an object in R (the process is known as coercing), which can be very useful; depending on what you’re doing, you might want to store a value as a different type. Coersion is done with the as functions; as.numeric to turn something into a numeric value, as.integer for an integer, and then as.logical and as.character for those respective types. Let’s use both techniques now, to clear up that facility_number vector: patient_data$facility_number &lt;- as.integer(patient_data$facility_number) str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 7 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: int 10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... As you can see, facility_number is now an integer vector, which lets us (amongst other things) do maths with it if we so choose - tremendously useful. Vectors are very powerful in their own right, and a useful object type for when you don’t want to be dealing with an entire data frame. They can be created in a couple of ways: # This creates a vector from scratch, using data in the code: a_vector &lt;- c(1,2,3,4,5) # This creates a vector from a data frame column patient_data_counts &lt;- patient_data$count Individual elements of a vector can be accessed using vectorname[n], where n is the Nth element. So if you want the fifth element from some_vector: some_vector[5]. We can add new vectors to data frames using a technique similar to how we modified them, and take advantage of a feature of R called “vectorisation”. The easiest way to think about it is Excel-related. You have your Excel spreadsheet, and you create a formula in one cell to add up all the values in that row. You want to do that for every row, so you drag the formula down the column and it copies the formula out, ending up with something like: A1 + A2 B1 + B2 C1 + C2 Vectorisation is just that - except it’s automated. No dragging and dropping required! Most R functions automatically work on entire vectors of data, and work when you have a vectors on one side but only a single value on the other. Let’s combine that with data frame modification to work out the percentage of the total patients admission number in each row: patient_data$percentage &lt;- (patient_data$admitted / sum(patient_data$admitted)) * 100 str(patient_data) Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;: 23578 obs. of 8 variables: $ year : int 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ... $ facility_number: int 10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ... $ facility : chr &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; &quot;ALAMEDA HOSPITAL, ALAMEDA&quot; ... $ type_of_control: chr &quot;District&quot; &quot;District&quot; &quot;District&quot; &quot;District&quot; ... $ age_group : chr &quot;10-19&quot; &quot;20-29&quot; &quot;30-39&quot; &quot;40-49&quot; ... $ admitted : int 62 124 140 243 411 415 486 961 7743 788 ... $ released : int 2 63 67 122 172 343 7 389 1136 531 ... $ percentage : num 0.000266 0.000532 0.000601 0.001043 0.001764 ... No drag and drop necessary; the patient_data data frame has a new column, percentage, which contains (for each row) the admitted count divided by the total count of admitted patients for the entire dataset, which we retrieved with sum. So we now know: How to read data into R from CSV files (and TSV files!); How to examine the structure of an R object; How do access individual columns of a data frame; How to modify those columns, and create new ones; How vectors work; How vectorisation works. Let’s move on to everyone’s favourite business data format: Microsoft Excel! 3.2 Importing Excel spreadsheets Microsoft Excel is widely used in a business context, and correspondingly the data it outputs (.xls and .xlsx files) are widely distributed. We should learn how to read them into R, which is a big step in shifting away from Excel-based workflows and ensures that you can consume data from more traditional sources without running into format incompatibilities. For this we’ll need another R package, readxl, which does just that - reads in Excel files. Let’s load it and read in a file that the package ships with (which we can find with the system.file function): library(readxl) excel_data &lt;- read_excel(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) head(excel_data) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa It’s another data frame - as expected, since excel spreadsheets are tabular just like CSV files are. One important note - Excel spreadsheets can contain multiple sheets, each of them a table. How do we handle that? If you look at the documentation for read_excel, you’ll see a sheet argument, which you can use to specify which sheet to read in. The excel_sheets function, meanwhile, gets you how many sheets an Excel file has. So we can combine them to work out how many sheets we’re looking at, and then read in that many. But what happens when you have to write code that reads in a variety of different files, some with different numbers of sheets? How do we write code that doesn’t mind how many sheets there are? How do we store the results of that code? A few new concepts and pieces of R, for this question. First-off is how we’re going to store the results. We’re looking at an unknown number of sheets, which means an unknown number of data frames. Luckily, data frames can be stored as components of a single object, just like vectors are components of data frames. That means instead of having an unknown number of objects, we just have one. For that kind of storage, we’re going to introduce a new object type in R: lists. Lists can have any length you want, and contain pretty much any object you want, too, which makes them extremely powerful. You can construct and use them with, or without names. # A list without names nameless_list &lt;- list(2, 3) # A list with names named_list &lt;- list(first_object = 2, second_object = 3) The difference is how you get objects inside the list, or change those objects: if the list has names, you can use the $ character we use with data frame contents. If it doesn’t, you have to use indices - getting the Nth element of the object by pointing to [[N]]. Taking the list examples we created above: nameless_list[[1]] [1] 2 named_list$first_object [1] 2 So now we know how to store the results. But how do we get them? First, we need to have a way of automatically knowing how many sheets there are. We can do that with another readxl function, excel_sheets, which lists all the sheets in an Excel file: sheet_names &lt;- excel_sheets(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) sheet_names [1] &quot;iris&quot; &quot;mtcars&quot; &quot;chickwts&quot; &quot;quakes&quot; Then we need a way of taking that vector of sheet names, and, for each one, reading in that sheet, and storing all the results in a list. The answer is the lapply function. lapply takes a list (or vector), X and, for each element of X, performs a specified operation using it. The results of each operation on each element is then returned in a list. Perfect! So what we want is: sheet_names &lt;- excel_sheets(path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) all_sheets &lt;- lapply(X = sheet_names, FUN = read_excel, path = system.file(&quot;extdata/datasets.xlsx&quot;, package = &quot;readxl&quot;)) If you run str on the results of that, what you should see is that all_sheets is a list, containing 4 data frames, all of which contain a pile of columns and rows just like our example at the beginning. These data frames can be extracted using the indices operation; all_sheets[[1]] gets you the first sheet, and so on, and so forth. So now we’ve learned how to read in Excel files. We’ve also learned: What lists are (and how they can be used to store a variety of objects); How to apply a function to each element of a list (or vector); How to get the results of that application. 3.3 Other files There are a few other file types you might encounter. One of the most common is tab-separated value files (TSVs); another, less common, is fixed-width files, where there’s an arbitrary amount of space between elements on each row to make sure every entry is the same width. 3.4 Database import To connect to a MySQL database, we’ll use the RMySQL package (as you may be noticing, R package developers love putting the letter R in their package names). As the name suggests, it integrates R with MySQL, providing code that lets you connect to a MySQL database and read data from it, turning that data into a data frame along the way. This process actually has quite a few steps: you need to: # Connect to a database database_connection &lt;- dbConnect(RMySQL::MySQL(), host = &quot;ensembldb.ensembl.org&quot;, user = &quot;anonymous&quot;, table = NULL) # Send a query response &lt;- dbSendQuery(database_connection, &quot;SHOW DATABASES&quot;) # Fetch the results of that query results &lt;- dbFetch(response) # Clear the results on the server end dbClearResult(response) # Close the connection dbDisconnect(database_connection) head(results) Database 1 information_schema 2 aedes_aegypti_core_48_1b 3 aedes_aegypti_core_49_1b 4 aedes_aegypti_core_50_1c 5 aedes_aegypti_core_51_1c 6 aedes_aegypti_core_52_1d That’s complicated, and also a total pain if you have to do it more than once - that’s a lot of code to write out every time, when all you’re really likely to be modifying is the query, or maybe table, for that particular host. The good news is that programmers are fundamentally lazy creatures, and so have worked out a way around this: “a pile of code that I keep having to repeat with only minor changes” is exactly what a function is. So now we’re going to learn how to create a function - one with two parameters, database and query, which will handle all of the commands we used above, just modifying database and query to whatever the user calling the function specifies. Defining a function, in some ways, looks a lot like defining any other object: you assign the output of a command to a name, it’s just that in this case the “output” is a chunk of code: times_two &lt;- function(x){ output &lt;- x * 2 return(output) } times_two(x = 10) [1] 20 There are only two new things here: the bit in curly braces is the body of the function - the code that the function contains, that is executed when you call the function. The return statement at the end, which is also new, simply tells the function “and here is the object you give back to the user when you are done”. So for database connecting, we need to wrap all the code we used in the database example into a function that accepts db and query as arguments, and returns the results of the database query. Simple enough: query_database &lt;- function(db, query){ # Make a connection database_connection &lt;- dbConnect(RMySQL::MySQL(), host = &quot;ensembldb.ensembl.org&quot;, user = &quot;anonymous&quot;, db = db) # Send a query response &lt;- dbSendQuery(database_connection, query) # Fetch the results of that query results &lt;- dbFetch(response) # Clear the results on the server end dbClearResult(response) # Close the connection dbDisconnect(database_connection) # Return the results! return(results) } tables &lt;- query_database(db = &quot;aedes_aegypti_core_51_1c&quot;, query = &quot;SHOW TABLES&quot;) head(tables) Tables_in_aedes_aegypti_core_51_1c 1 alt_allele 2 analysis 3 analysis_description 4 assembly 5 assembly_exception 6 attrib_type And we can keep reusing query_database with whatever database (or query!) we want. 3.5 Exporting data A related topic to data import - albeit, one you won’t have to use quite yet - is data export. It’s pretty common to need to save the results of whatever work you’re doing, either so you can pick it up later, hand it off to other people, or use it in other processes. There are a couple of ways to do that. If you’re saving simple, tabular data - data frames, say - we can use the readr package again. As well as read_csv and read_tsv, there’s also write_csv and write_tsv. : patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) write_csv(x = patient_data, file = &quot;patient_data.csv&quot;) You should now have a file called “patient_data.csv” in whatever directory you’re working from. It’s properly formatted and can be read back into R, or into Excel, or into whatever tool you choose. Obviously, non-tabular objects (such as lists) can’t be saved into a CSV: how would R, or any other program, know what to do with them? This can present a problem when trying to share work with others, or save it for your own reuse. Luckily, R has its own data format, .RData files, which can store representations of any R object you want and then be loaded seamlessly back into R (the limitation is it probably won’t work with other languages or software). These files can also contain however many objects you want, of many different types. They’re created with the save function: a_list &lt;- list(a = 12, b = 6) a_vector &lt;- c(1, 2, 3, 4) save(a_list, a_vector, file = &quot;an_rdata_file.RData&quot;) You should now have an_rdata_file.Rdata saved to disk. It can be read in with the load function, which will restore the objects with whatever names they had when they were saved. The answer is usually data cleaning. People tend to act as if analysis is 50% studious statistical testing and 50% visualisation and reporting, but this is a damned dirty lie. In practice, it’s 80% cleaning the data so it will work, 20% statistics, and a further 20% visualisation - summing to over 100 because you didn’t realise quite how much of a pain the cleaning would be. "],
["text-cleaning.html", "4 Text-cleaning", " 4 Text-cleaning "],
["dates-and-times.html", "5 Dates and times", " 5 Dates and times "],
["reshaping-data.html", "6 Reshaping data", " 6 Reshaping data One essential part of data cleaning is reshaping the actual structure of your data. Sometimes as researchers we are lucky enough to have data that looks exactly like what we want. Every row is a single observation. Every column contains one, and just one, thing we care about. This is referred to as tidy data (a concept you can read more about here). We’ll use the original patient dataset as an example. Let’s take a look at it: patient_data &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) head(patient_data) year facility_number facility type_of_control age_group admitted released 1 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL, ALAMEDA District 60-69 415 343 There are a few ways in which this data is ‘untidy’. First-off, the facility and facility location are in the same column - we might want to have them as different fields. Second, we’ve got multiple types of data (patient admissions, and patient releases) in different columns. This is good for some operations, like calculating percentages, but can be a tremendous pain with others. We may need to tidy the data, perform operations on it, untidy it for others, and then back again. So we want to be able to split a column, merge patient admissions and patient releases into one key-value structure, and learn how to un-merge it just in case we have to. We can do all of these with the tidyr package. First: column-splitting. We’ve got that facility field, containing both the facility name and location, and we want two columns (facility_name and facility_location). For that we can use the separate function from tidyr, which does just that: separates the contents of a specified column, based on a user-defined separator, into multiple new columns (with user-specified names). Let’s use it now: library(tidyr) split_data &lt;- separate(data = patient_data, col = facility, into = c(&quot;facility_name&quot;, &quot;facility_location&quot;), sep = &quot;, &quot;) head(split_data) year facility_number facility_name facility_location type_of_control age_group admitted released &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 415 343 We’ve split facility into facility_name and facility_location. Pretty neat! Still leaves us with what to do about the multiple value columns, though. The answer comes in the form of gather, another tidyr function, which takes a data frame, the names to give the key and value columns, and columns to treat as id columns that aren’t touched. It then splits the rest: gathered_data &lt;- gather(data = split_data, key = &quot;type&quot;, value = &quot;count&quot;, admitted, released) head(gathered_data) year facility_number facility_name facility_location type_of_control age_group type count &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 admitted 62 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 admitted 124 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 admitted 140 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 admitted 243 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 admitted 411 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 admitted 415 If we want to reverse that operation, we can use spread: spread_data &lt;- spread(data = gathered_data, key = type, value = count) head(spread_data) year facility_number facility_name facility_location type_of_control age_group admitted released &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 10-19 62 2 2 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 20-29 124 63 3 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 30-39 140 67 4 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 40-49 243 122 5 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 50-59 411 172 6 2009 010735 ALAMEDA HOSPITAL ALAMEDA District 60-69 415 343 …and at that point, we’re right back where we started. "],
["data-manipulation.html", "7 Data manipulation", " 7 Data manipulation Beyond tidying data, another common need is to aggregate it. Raw data, split by as many variables as possible, gives us the widest possible amount of information, but that doesn’t mean it’s easy to visualise (patient_data has 23,578 observations, for example. Nobody wants a 23,000 point graphic) or understand. Data manipulation is the practice of reshaping, aggregating or splitting data to make it easier to use for a particular task. If you’re exploring data, that might look like aggregating it at a very high level to get a general feel for what things look like. If you’re visualising it, that might look like calculating particular metrics for entire groups of observations, and getting a data frame out at the end you can turn into a graph. In these cases, and a wide range of other cases, the answer is another R package: dplyr. dplyr provides a “grammar” of data manipulation: a set of functions that can be strung together into almost a sentence that describes what’s being done to the data. For example, if we wanted to summarise, for each facility, the number of admitted patients in the 10-19 age group, we would filter the dataset to 10-19, group the entries by facility name, and summarize the dataset to get the mean. This language-like construct is aided by a new operator that we’ll introduce: %&gt;%, or the “pipe” operator. Normally, R consists of a series of statements with assignment operations in them: start_data &lt;- &quot;foo&quot; result_of_first_step &lt;- do_thing(start_data) result_of_second_step &lt;- do_second_thing(result_of_first_step) final_result &lt;- do_third_thing(result_of_second_step) The pipe operator lets us “chain” operations together. If you put pipe operatiors between function calls, it will automatically pass the result of the first call into the second, and second to the third, and so on until it runs out of pipe operators (and function calls) and returns the result. This means instead of assigning things 3 times for 3 operations, we assign things..once: start_data &lt;- &quot;foo&quot; final_result &lt;- do_thing(start_data) %&gt;% do_second_thing %&gt;% do_third_thing The code is much cleaner and much easier to read, and when you add the “language”-like aspect of dplyr and tidyr in, it becomes almost a sentence. Let’s demonstrate by taking the patient data dataset, and then working out the average number of 10-19 year olds admitted to facilities a year: facility_admission_average &lt;- read_csv(file = &quot;https://www.ironholds.org/resources/misc/patient_data.csv&quot;) %&gt;% filter(age_group == &quot;10-19&quot;) %&gt;% group_by(year) %&gt;% summarise(admission_average = mean(admitted)) head(facility_admission_average) year admission_average &lt;int&gt; &lt;dbl&gt; 1 2009 418.5859 2 2010 407.1007 3 2011 390.5659 4 2012 378.8952 5 2013 356.8860 6 2014 350.0652 And just like that we’ve gone from raw data to averaged hospital admissions, by year, for a particular age group - although we probably should have rounded the results. I’m not sure if you can admit half a patient. Certainly not without a rather grim prognosis. "],
["building-your-first-app.html", "8 Building your first app", " 8 Building your first app "],
["user-interface-design.html", "9 User Interface design", " 9 User Interface design "],
["custom-inputs-and-outputs.html", "10 Custom inputs and outputs", " 10 Custom inputs and outputs "],
["reactive-expressions.html", "11 Reactive expressions", " 11 Reactive expressions "],
["shiny-dashboarding.html", "12 Shiny dashboarding", " 12 Shiny dashboarding "],
["code-style.html", "13 Code style", " 13 Code style "],
["version-control.html", "14 Version control", " 14 Version control "],
["testing-and-staging.html", "15 Testing and staging", " 15 Testing and staging "]
]
