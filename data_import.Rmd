# Data Import and Export

Obviously the first step in data analysis is getting the data, so let's start there! Datasets come in lots of different formats, so we'll cover the ones you most commonly see in a business context, namely:

1. Comma-separated and tab-separated values files (CSV/TSV);
2. Microsoft Excel spreadsheets;
3. MySQL and similar databases.

## Importing from CSVs

CSVs and TSVs are the most common file format for data, and the good news there is that as a consequence, R has a whole host of tools for reading them in. Let's open by loading one of them: the `readr` add-on I had you install earlier:

```{r, eval=FALSE}
library(readr)
```

So, what's this code doing? First, `library`; that's a function, which is a self-contained chunk of code that's packaged and made available so you can use it as many times as you want without having to write the code out entirely each time. `readr` is the input provided to `library`. So it works out as `do_this_thing(to_this_thing)`.

The `library` function loads *packages* - the add-ons I was talking about earlier - so you can refer to the code they contain when you're working. In this case it's loading `readr`, a package specifically designed for reading in and writing out a wide range of file formats, including CSVs and TSVs.

When you see a function name, you can usually access documentation about it; remember the `help` pane I mentioned earlier? If you type the function name into the console, preceded by `?`, the help pane will pop up with documentation for that function - what it's used for, what values it accepts, and generally how it works.

So we have our file-reading code. Now let's read in the file, with:

```{r, eval=FALSE}
patient_data <- read_csv(file = "https://www.ironholds.org/resources/misc/patient_data.csv")
```

Let's break down what's going on in that code, going from right to left. First we have a piece of text - a URL, in fact - in quote marks. The quote marks identify the text as a `string`; something that R should treat just as text, not as code to be executed. Then we have that URL being associated with "file", the entire thing wrapped in parentheses, preceded by `read_csv` which, as we've seen from loading the `readr` package, is a function call.

So what we have is a call to run `read_csv`, with the "file" argument being that URL - so it's reading a CSV from that URL! The argument, as the name `file` suggests, doesn't have to be a URL; it can also be a file on your local computer.

Then we shift left and run into `<-`. That's R's *assignment operator*; it assigns the results of whatever code is on the right of the operator, to a name on the left of it (in this case, `patient_data`). So the code, in plain English, is "read in the CSV at this URL and assign the contents of it to the name `patient_data`". If the file was a TSV, the process would look exactly the same; you'd just use `read_tsv` instead of `read_csv`.

We call the result (the output of code, tied to a name) an *object*. You can see the contents of an object just by typing its name into the console, but that shows the entire thing, and this dataset is rather large; instead, let's use the `head` function, which just shows the top few rows (the head of the data):

```{r, eval=FALSE}
head(patient_data)


  year facility_number                  facility type_of_control age_group admitted released
1 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     10-19       62        2
2 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     20-29      124       63
3 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     30-39      140       67
4 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     40-49      243      122
5 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     50-59      411      172
6 2009          010735 ALAMEDA HOSPITAL, ALAMEDA        District     60-69      415      343
```

As you can see, it's tabular, just like the CSV - columns of values, rows of observations, and row numbers (from 1 to 6). What you're looking at is a "data frame", the most common type of object in R: it's designed to hold tabular data, since tabular data is what most statisticians and data scientists rely on.

We can get a more detailed look at it with the `str` function (which means "structure" and does exactly what it says on the tin):

```{r, eval=FALSE}
str(patient_data)

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	23578 obs. of  7 variables:
 $ year           : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...
 $ facility_number: chr  "010735" "010735" "010735" "010735" ...
 $ facility       : chr  "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" ...
 $ type_of_control: chr  "District" "District" "District" "District" ...
 $ age_group      : chr  "10-19" "20-29" "30-39" "40-49" ...
 $ admitted       : int  62 124 140 243 411 415 486 961 7743 788 ...
 $ released       : int  2 63 67 122 172 343 7 389 1136 531 ...
```

There's a lot going on here, so let's break it down:

1. First, "classes"; this is analogous to the object type. We don't have to dig into exactly how the system works (although feel free to ask me about it via email or do further research); the important thing to note is that one of the classes is `data.frame`, identifying this as, well, a data frame!
2. We then have "23578 obs. of 7 variables"; there are 23,578 rows, and 7 columns (which, in R, are known as `vectors` - more on those later). You can get these bits of information distinctly, through the `nrow` and `ncol` functions.
3. We then have a sort of flipped version of the `head` output - showing each vector, from the first to the last, down. There are the vector names (`year`, `facility_number`, `facility`), examples of the entries in those columns, and then these little strings "int" and "chr". These represents the "type" of the column - the kind of data it can contain. `int` vectors contain whole numbers; `chr` (or "character") vectors contain strings. Other common types to see are `logical` (or `logi`), which contains true and false values, and `numeric`/`num` columns, which contain non-whole numbers.

So now we have our data read into R. It's not quite perfect, though; the `facility_number` vector is meant to be, well, a column of numbers, but according to `str` it's actually a character vector. This offers us an opportunity to explore modifying data frames, and switching between types.

One of the reasons data frames are powerful is that you can access, change and use individual vectors, as well as the data frame as a whole; this is done by calling, instead of `data_frame_name`, `data_frame_name$column_name`.

Distinctly from that, it is possible to change the type of an object in R (the process is known as `coercing`), which can be very useful; depending on what you're doing, you might want to store a value as a different type. Coersion is done with the `as` functions; `as.numeric` to turn something into a numeric value, `as.integer` for an integer, and then `as.logical` and `as.character` for those respective types.

Let's use both techniques now, to clear up that `facility_number` vector:

```{r, eval=FALSE}
patient_data$facility_number <- as.integer(patient_data$facility_number)

str(patient_data)

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	23578 obs. of  7 variables:
 $ year           : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...
 $ facility_number: int  10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ...
 $ facility       : chr  "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" ...
 $ type_of_control: chr  "District" "District" "District" "District" ...
 $ age_group      : chr  "10-19" "20-29" "30-39" "40-49" ...
 $ admitted       : int  62 124 140 243 411 415 486 961 7743 788 ...
 $ released       : int  2 63 67 122 172 343 7 389 1136 531 ...
```

As you can see, `facility_number` is now an integer vector, which lets us (amongst other things) do maths with it if we so choose - tremendously useful.

Vectors are very powerful in their own right, and a useful object type for when you don't want to be dealing with an entire data frame. They can be created in a couple of ways:

```{r, eval=FALSE}
# This creates a vector from scratch, using data in the code:
a_vector <- c(1,2,3,4,5)

# This creates a vector from a data frame column
patient_data_counts <- patient_data$count
```

Individual elements of a vector can be accessed using `vectorname[n]`, where `n` is the Nth element. So if you want the fifth element from `some_vector`: `some_vector[5]`.

We can add new vectors to data frames using a technique that is similar to the one we used to modify them. We can also take advantage of a feature of R called "vectorisation", which is most-easily thought about with an Excel analogy. You have your Excel spreadsheet, and you create a formula in one cell to add up all the values in that row. You want to do that for every row, so you drag the formula down the column and it copies the formula out, ending up with something like:

```
A1 + A2
B1 + B2
C1 + C2
```

Vectorisation is just that - except it's automated. No dragging and dropping required! Most R functions automatically work on entire vectors of data, and work when you have a vectors on one side but only a single value on the other. Let's combine that with data frame modification to work out the *percentage* of the total patients admission number in each row:

```{r, eval=FALSE}
patient_data$percentage <- (patient_data$admitted / sum(patient_data$admitted)) * 100

str(patient_data)

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	23578 obs. of  8 variables:
 $ year           : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...
 $ facility_number: int  10735 10735 10735 10735 10735 10735 10735 10735 10739 10739 ...
 $ facility       : chr  "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" "ALAMEDA HOSPITAL, ALAMEDA" ...
 $ type_of_control: chr  "District" "District" "District" "District" ...
 $ age_group      : chr  "10-19" "20-29" "30-39" "40-49" ...
 $ admitted       : int  62 124 140 243 411 415 486 961 7743 788 ...
 $ released       : int  2 63 67 122 172 343 7 389 1136 531 ...
 $ percentage     : num  0.000266 0.000532 0.000601 0.001043 0.001764 ...
```

No drag and drop necessary; the `patient_data` data frame has a new column, `percentage`, which contains (for each row) the `admitted` count divided by the total count of admitted patients for the entire dataset, which we retrieved with `sum`.

So we now know:

1. How to read data into R from CSV files (and TSV files!);
2. How to examine the structure of an R object;
2. How do access individual columns of a data frame;
3. How to modify those columns, and create new ones;
4. How vectors work;
5. How vectorisation works.

Let's move on to everyone's favourite business data format: Microsoft Excel!

## Importing Excel spreadsheets

Microsoft Excel is widely used in a business context, and correspondingly the data it outputs (`.xls` and `.xlsx` files) are widely distributed. We should learn how to read them into R, which is a big step in shifting away from Excel-based workflows and ensures that you can consume data from more traditional sources without running into format incompatibilities.

For this we'll need another R package, `readxl`, which does just that - reads in Excel files. Let's load it and read in a file that the package comes with (which we can find with the `system.file` function):

```{r, eval=FALSE}
library(readxl)
excel_data <- read_excel(path = system.file("extdata/datasets.xlsx", package = "readxl"))
head(excel_data)

  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
```

It's another data frame - as expected, since Excel spreadsheets are tabular just like CSV files are. One important note - Excel spreadsheets can contain multiple sheets, *each* of them a table. How do we handle that?

If you look at the documentation for `read_excel`, you'll see a `sheet` argument, which you can use to specify which sheet to read in. The `excel_sheets` function, meanwhile, gets you how many sheets an Excel file has. So we can combine them to work out how many sheets we're looking at, and then read in that many.

But what happens when you have to write code that reads in a variety of different files, some with different numbers of sheets? How do we write code that doesn't mind how many sheets there are? How do we store the results of that code?

A few new concepts and pieces of R, for these questions. First-off is how we're going to store the results. We're looking at an unknown number of sheets, which means an unknown number of data frames. Luckily, data frames can be stored as components of a single object, just like vectors are components of data frames. That means instead of having an unknown number of objects, we just have one.

For that kind of storage, we're going to introduce a new object type in R: `lists`. Lists can have any length you want, and contain pretty much any object you want, too, which makes them extremely powerful. You can construct and use them with, or without names.

```{r, eval=FALSE}

# A list without names
nameless_list <- list(2, 3)

# A list with names
named_list <- list(first_object = 2, second_object = 3)
```

The difference is how you get objects inside the list, or change those objects: if the list has names, you can use the `$` character we use with data frame contents. If it doesn't, you have to use indices - getting the Nth element of the object by pointing to `[[N]]`. Taking the list examples we created above:

```{r, eval=FALSE}
nameless_list[[1]]
[1] 2

named_list$first_object
[1] 2
```

So now we know how to store the results. But how do we get them?

First, we need to have a way of automatically knowing how many sheets there are. We can do that with another `readxl` function, `excel_sheets`, which lists all the sheets in an Excel file:

```{r, eval=FALSE}
sheet_names <- excel_sheets(path = system.file("extdata/datasets.xlsx", package = "readxl"))
sheet_names

[1] "iris"     "mtcars"   "chickwts" "quakes"
```

Then we need a way of taking that vector of sheet names, and, for each one, reading in that sheet, and storing all the results in a list. The answer is the `lapply` function.

`lapply` takes a list (or vector), `X` and, for each element of `X`, performs a specified operation using it. The results of each operation on each element is then returned in a list. Perfect! So what we want is:

```{r, eval=FALSE}
sheet_names <- excel_sheets(path = system.file("extdata/datasets.xlsx", package = "readxl"))
all_sheets <- lapply(X = sheet_names, FUN = read_excel,
                     path = system.file("extdata/datasets.xlsx", package = "readxl"))
```

If you run `str` on the results of that, what you *should* see is that `all_sheets` is a list, containing 4 data frames, all of which contain a pile of columns and rows just like our example at the beginning. These data frames can be extracted using the indices operation; `all_sheets[[1]]` gets you the first sheet, and so on, and so forth.

So now we've learned how to read in Excel files. We've also learned:

1. What lists are (and how they can be used to store a variety of objects);
2. How to apply a function to each element of a list (or vector);
3. How to get the results of that application.

## Other files

There are a few other file types you might encounter. One of the most common is *tab*-separated value files (TSVs); another, less common, is fixed-width files, where there's an arbitrary amount of space between elements on each row to make sure every entry is the same width.

## Database import

To connect to a MySQL database, we'll use the `RMySQL` package (as you may be noticing, R package developers *love* putting the letter R in their package names). As the name suggests, it integrates R with MySQL, providing code that lets you connect to a MySQL database and read data from it, turning that data into a data frame along the way.

This process actually has quite a few steps: you need to:

```{r, eval=FALSE}
# Connect to a database
database_connection <- dbConnect(RMySQL::MySQL(), host = "ensembldb.ensembl.org", user = "anonymous",
                                 table = NULL)

# Send a query
response <- dbSendQuery(database_connection, "SHOW DATABASES")

# Fetch the results of that query
results <- dbFetch(response)

# Clear the results on the server end
dbClearResult(response)

# Close the connection
dbDisconnect(database_connection)

head(results)
                  Database
1       information_schema
2 aedes_aegypti_core_48_1b
3 aedes_aegypti_core_49_1b
4 aedes_aegypti_core_50_1c
5 aedes_aegypti_core_51_1c
6 aedes_aegypti_core_52_1d
```

That's complicated, and also a total pain if you have to do it more than once - that's a lot of code to write out every time, when all you're really likely to be modifying is the query, or maybe table, for that particular host.

The good news is that programmers are fundamentally lazy creatures, and so have worked out a way around this: "a pile of code that I keep having to repeat with only minor changes" is exactly what a function is. So now we're going to learn how to create a function - one with two parameters, `database` and `query`, which will handle all of the commands we used above, just modifying `database` and `query` to whatever the user calling the function specifies.

Defining a function, in some ways, looks a lot like defining any other object: you assign the output of a command to a name, it's just that in this case the "output" is a chunk of code:

```{r, eval=FALSE}
times_two <- function(x){
  output <- x * 2
  return(output)
}

times_two(x = 10)
[1] 20
```

There are only two new things here: the bit in curly braces is the `body` of the function - the code that the function contains, that is executed when you call the function. The `return` statement at the end, which is also new, simply tells the function "and here is the object you give back to the user when you are done".

So for database connecting, we need to wrap all the code we used in the database example into a function that accepts `db` and `query` as arguments, and returns the results of the database query. Simple enough:

```{r, eval=FALSE}

query_database <- function(db, query){
  
  # Make a connection
  database_connection <- dbConnect(RMySQL::MySQL(), host = "ensembldb.ensembl.org", user = "anonymous",
                                   db = db)

  # Send a query
  response <- dbSendQuery(database_connection, query)
  
  # Fetch the results of that query
  results <- dbFetch(response)
  
  # Clear the results on the server end
  dbClearResult(response)
  
  # Close the connection
  dbDisconnect(database_connection)
  
  # Return the results!
  return(results)
}

tables <- query_database(db = "aedes_aegypti_core_51_1c", query = "SHOW TABLES")
head(tables)

  Tables_in_aedes_aegypti_core_51_1c
1                         alt_allele
2                           analysis
3               analysis_description
4                           assembly
5                 assembly_exception
6                        attrib_type
```

And we can keep reusing `query_database` with whatever database (or query!) we want.

## Exporting data
A related topic to data import - albeit, one you won't have to use quite yet - is data export. It's pretty common to need to save the results of whatever work you're doing, either so you can pick it up later, hand it off to other people, or use it in other processes.

There are a couple of ways to do that. If you're saving simple, tabular data - data frames, say - we can use the `readr` package again. As well as `read_csv` and `read_tsv`, there's also `write_csv` and `write_tsv`. :

```{r, eval=FALSE}
patient_data <- read_csv(file = "https://www.ironholds.org/resources/misc/patient_data.csv")

write_csv(x = patient_data, file = "patient_data.csv")
```

You should now have a file called "patient_data.csv" in whatever directory you're working from. It's properly formatted and can be read back into R, or into Excel, or into whatever tool you choose.

Obviously, non-tabular objects (such as lists) can't be saved into a CSV: how would R, or any other program, know what to do with them? This can present a problem when trying to share work with others, or save it for your own reuse.

Luckily, R has its own data format, `.RData` files, which can store representations of any R object you want and then be loaded seamlessly back into R (the limitation is it probably won't work with other languages or software). These files can also contain however many objects you want, of many different types. They're created with the `save` function:

```{r, eval=FALSE}
a_list <- list(a = 12, b = 6)
a_vector <- c(1, 2, 3, 4)
save(a_list, a_vector, file = "an_rdata_file.RData")
```

You should now have `an_rdata_file.Rdata` saved to disk. It can be read in with the `load` function, which will restore the objects with whatever names they had when they were saved.
